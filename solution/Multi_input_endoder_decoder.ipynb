{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjJxiyGl70hZ"
      },
      "source": [
        "graph TD\n",
        "    A[Input Image] --> B[Vision Encoder]\n",
        "    C[Text Query] --> D[Text Encoder]\n",
        "    B --> E[Cross-Attention Transformer]\n",
        "    D --> E\n",
        "    E --> F[Decoder]\n",
        "    F --> G[Output Text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1nbhSPbo70ha"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import ViTModel\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Tuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model & Supporter Classes"
      ],
      "metadata": {
        "id": "pgz_2fatG1FL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OutputTokenizer:\n",
        "    def __init__(self, unit_list: List[str]):\n",
        "        self.unit_list = unit_list\n",
        "        self.unit_to_id = {unit: i for i, unit in enumerate(unit_list)}\n",
        "        self.id_to_unit = {i: unit for i, unit in enumerate(unit_list)}\n",
        "        self.num_units = len(unit_list)\n",
        "\n",
        "        # Special tokens\n",
        "        self.PAD_token = 0\n",
        "        self.SOS_token = 1\n",
        "        self.EOS_token = 2\n",
        "        self.UNK_token = 3 # for Unknown tokens\n",
        "\n",
        "        # Vocabulary size: special tokens + digits + decimal point + units\n",
        "        self.vocab_size = 4 + 10 + 1 + self.num_units\n",
        "\n",
        "    def tokenize(self, text: str, max_length: int = 50) -> List[int]:\n",
        "        # Split the input into number and unit\n",
        "        match = re.match(r'(\\d+\\.?\\d*)\\s*(\\w+)', text.strip())\n",
        "        if not match:\n",
        "            return [self.SOS_token, self.UNK_token, self.EOS_token] + [self.PAD_token] * (max_length - 3)\n",
        "\n",
        "        number, unit = match.groups()\n",
        "\n",
        "        # Tokenize the number\n",
        "        number_tokens = [int(digit) + 4 for digit in number if digit.isdigit()]\n",
        "        if '.' in number:\n",
        "            number_tokens.insert(number.index('.'), 14)  # 14 is the token for decimal point\n",
        "\n",
        "        # Tokenize the unit\n",
        "        unit_token = self.unit_to_id.get(unit, self.UNK_token)\n",
        "\n",
        "        # Combine tokens\n",
        "        tokens = [self.SOS_token] + number_tokens + [unit_token + 15] + [self.EOS_token]\n",
        "\n",
        "        # Pad or truncate to max_length\n",
        "        if len(tokens) < max_length:\n",
        "            tokens += [self.PAD_token] * (max_length - len(tokens))\n",
        "        else:\n",
        "            tokens = tokens[:max_length-1] + [self.EOS_token]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def detokenize(self, tokens: List[int]) -> str:\n",
        "        number = ''\n",
        "        unit = ''\n",
        "        for token in tokens:\n",
        "            if token == self.SOS_token or token == self.PAD_token:\n",
        "                continue\n",
        "            elif token == self.EOS_token:\n",
        "                break\n",
        "            elif 4 <= token <= 13:\n",
        "                number += str(token - 4)\n",
        "            elif token == 14:\n",
        "                number += '.'\n",
        "            elif token >= 15:\n",
        "                unit = self.id_to_unit.get(token - 15, 'UNK')\n",
        "\n",
        "        return f\"{number} {unit}\"\n",
        "\n",
        "    def decode(self, token_ids: List[int]) -> str:\n",
        "        return self.detokenize(token_ids)\n"
      ],
      "metadata": {
        "id": "fl8sFTfL-5-N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EntityPredictorVisionBasedModel(nn.Module):\n",
        "    def __init__(self, num_entity_names, num_group_ids, vocab_size, max_length=50):\n",
        "        super().__init__()\n",
        "        self.image_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "        self.entity_embedding = nn.Embedding(num_entity_names, 768)\n",
        "        self.group_embedding = nn.Embedding(num_group_ids, 768)\n",
        "\n",
        "        self.cross_attention = nn.MultiheadAttention(embed_dim=768, num_heads=8)\n",
        "\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model=768, nhead=8),\n",
        "            num_layers=6\n",
        "        )\n",
        "\n",
        "        self.output_layer = nn.Linear(768, vocab_size)\n",
        "        self.embedding = nn.Embedding(vocab_size, 768)\n",
        "\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def forward(self, images, entity_names, group_ids, target_texts=None):\n",
        "        # Encode images\n",
        "        image_features = self.image_encoder(images).last_hidden_state\n",
        "\n",
        "        # Encode entity names and group IDs\n",
        "        entity_embeddings = self.entity_embedding(entity_names)\n",
        "        group_embeddings = self.group_embedding(group_ids)\n",
        "\n",
        "        # Combine entity and group embeddings\n",
        "        query = entity_embeddings + group_embeddings\n",
        "        query = query.unsqueeze(1).repeat(1, self.max_length, 1)\n",
        "\n",
        "        # Cross-attention\n",
        "        image_features = image_features.transpose(0, 1)\n",
        "        query = query.transpose(0, 1)\n",
        "        attended_features, _ = self.cross_attention(query, image_features, image_features)\n",
        "\n",
        "        batch_size = images.size(0)\n",
        "        seq_length = self.max_length\n",
        "\n",
        "        if self.training and target_texts is not None:\n",
        "            # During training, use teacher forcing\n",
        "            decoder_inputs = self.embedding(target_texts[:, :-1])\n",
        "            decoder_outputs = self.decode(decoder_inputs, attended_features)\n",
        "        else:\n",
        "            # During validation/inference, generate the full sequence\n",
        "            decoder_input = torch.zeros((batch_size, 1, 768), device=images.device)\n",
        "            decoder_outputs = []\n",
        "\n",
        "            for _ in range(seq_length):\n",
        "                step_output = self.decode(decoder_input, attended_features)\n",
        "                decoder_outputs.append(step_output)\n",
        "                next_token = step_output.argmax(dim=-1)\n",
        "                decoder_input = self.embedding(next_token)\n",
        "\n",
        "            decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "\n",
        "        return decoder_outputs\n",
        "\n",
        "    def decode(self, decoder_input, attended_features):\n",
        "        # Transpose for decoder\n",
        "        decoder_input = decoder_input.transpose(0, 1)\n",
        "\n",
        "        # Decode\n",
        "        decoder_output = self.decoder(decoder_input, attended_features)\n",
        "\n",
        "        # Generate output probabilities\n",
        "        output_probs = self.output_layer(decoder_output.transpose(0, 1))\n",
        "\n",
        "        return output_probs\n",
        "\n",
        "    def generate(self, images, entity_names, group_ids):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            output_probs = self(images, entity_names, group_ids)\n",
        "            generated_tokens = output_probs.argmax(dim=-1)\n",
        "        return generated_tokens"
      ],
      "metadata": {
        "id": "pQL0tIeB74Cb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "def entity_to_index(entity_names):\n",
        "    unique_entities = sorted(set(entity_names))\n",
        "    entity_to_idx = {entity: idx for idx, entity in enumerate(unique_entities)}\n",
        "    return [entity_to_idx[entity] for entity in entity_names]\n",
        "\n",
        "def group_to_index(group_ids):\n",
        "    unique_groups = sorted(set(group_ids))\n",
        "    group_to_idx = {group: idx for idx, group in enumerate(unique_groups)}\n",
        "    return [group_to_idx[group] for group in group_ids]\n"
      ],
      "metadata": {
        "id": "akUBHLaY8J1S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProductImageDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, transform=None, max_length=50):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.transform = transform or transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            row = self.df.iloc[idx]\n",
        "\n",
        "            # Load image from URL\n",
        "            response = requests.get(row['image_link'], timeout=10)\n",
        "            response.raise_for_status()\n",
        "            img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "            img = self.transform(img)\n",
        "\n",
        "            # Prepare other inputs\n",
        "            entity_name = torch.tensor(row['entity_name_index'], dtype=torch.long)\n",
        "            group_id = torch.tensor(row['group_id_index'], dtype=torch.long)\n",
        "\n",
        "            # Prepare target\n",
        "            target = row['entity_value']\n",
        "            target = self.tokenizer.tokenize(target, max_length=self.max_length)\n",
        "            target = torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "            return img, entity_name, group_id, target\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data at index {idx}: {str(e)}\")\n",
        "            # Return a default value with correct shapes\n",
        "            return torch.zeros((3, 224, 224)), torch.tensor(0, dtype=torch.long), torch.tensor(0, dtype=torch.long), torch.tensor([0] * self.max_length, dtype=torch.long)"
      ],
      "metadata": {
        "id": "OBQ-vgiiFfhT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "N7G5z2N9GGIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuvghQbuGALl",
        "outputId": "f2789dd8-9254-4bf8-aba3-f407c14d7f44"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7909d438cad0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fod2VdFdGAkf",
        "outputId": "f9acebc4-c66c-44f4-f220-ed02e3c3ce77"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/Dhyanesh-Panchal/amazon_ml_2024_deepinsight/refs/heads/master/student_resource%203/dataset/filtered_train.csv')\n",
        "df.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "bNTQSt1SANbQ",
        "outputId": "acbfd778-40ea-4940-818e-59db8564b42d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index                                         image_link  group_id  \\\n",
              "0      0  https://m.media-amazon.com/images/I/61I9XdN6OF...    748919   \n",
              "1      1  https://m.media-amazon.com/images/I/71gSRbyXmo...    916768   \n",
              "\n",
              "   entity_name entity_value  \n",
              "0  item_weight   500.0 gram  \n",
              "1  item_volume      1.0 cup  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2f61d5bf-152c-4e90-947f-c5f57e2fb3bf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>image_link</th>\n",
              "      <th>group_id</th>\n",
              "      <th>entity_name</th>\n",
              "      <th>entity_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>https://m.media-amazon.com/images/I/61I9XdN6OF...</td>\n",
              "      <td>748919</td>\n",
              "      <td>item_weight</td>\n",
              "      <td>500.0 gram</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>https://m.media-amazon.com/images/I/71gSRbyXmo...</td>\n",
              "      <td>916768</td>\n",
              "      <td>item_volume</td>\n",
              "      <td>1.0 cup</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f61d5bf-152c-4e90-947f-c5f57e2fb3bf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2f61d5bf-152c-4e90-947f-c5f57e2fb3bf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2f61d5bf-152c-4e90-947f-c5f57e2fb3bf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e8ba496e-b400-45ff-aa5b-ed5b04ed7e79\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e8ba496e-b400-45ff-aa5b-ed5b04ed7e79')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e8ba496e-b400-45ff-aa5b-ed5b04ed7e79 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['entity_value'])"
      ],
      "metadata": {
        "id": "kXe2VT4YGNt-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets sample a small subset"
      ],
      "metadata": {
        "id": "aLXqZDtJeZL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_no = 1\n",
        "train_batch_size = 25000"
      ],
      "metadata": {
        "id": "zZjO-WvKmkMJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[(train_batch_no-1)*train_batch_size : (train_batch_no)*train_batch_size]"
      ],
      "metadata": {
        "id": "Q3g57hikeduP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck4X6oSmGTPX",
        "outputId": "3969a8cb-f454-48ae-a3a8-2e0bb35b4c05"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 25000 entries, 0 to 24999\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   index         25000 non-null  int64 \n",
            " 1   image_link    25000 non-null  object\n",
            " 2   group_id      25000 non-null  int64 \n",
            " 3   entity_name   25000 non-null  object\n",
            " 4   entity_value  25000 non-null  object\n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 976.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare entity names and group IDs\n",
        "entity_to_index = {entity: idx for idx, entity in enumerate(df['entity_name'].unique())}\n",
        "group_to_index = {group: idx for idx, group in enumerate(df['group_id'].unique())}"
      ],
      "metadata": {
        "id": "wItxKFaOAI6V"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['entity_name_index'] = df['entity_name'].map(entity_to_index)\n",
        "df['group_id_index'] = df['group_id'].map(group_to_index)"
      ],
      "metadata": {
        "id": "DMGaO97qGDeX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare unit list\n",
        "df['entity_value_unit'] = df['entity_value'].apply(lambda x: \" \".join(x.split(\" \")[1:]))\n",
        "unit_list = df['entity_value_unit'].unique().tolist()"
      ],
      "metadata": {
        "id": "ZPIGeOF-GXGA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unit_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbi3VFWOAI5M",
        "outputId": "bd48f2b2-d786-4d8f-8384-46d883fce9b3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gram',\n",
              " 'cup',\n",
              " 'milligram',\n",
              " 'kilogram',\n",
              " 'ounce',\n",
              " 'gallon',\n",
              " 'volt',\n",
              " 'watt',\n",
              " 'pound',\n",
              " 'millilitre',\n",
              " 'cubic foot',\n",
              " 'fluid ounce',\n",
              " 'ton',\n",
              " 'decilitre',\n",
              " 'cubic inch',\n",
              " 'litre',\n",
              " 'microgram',\n",
              " 'centimetre',\n",
              " 'quart',\n",
              " 'horsepower',\n",
              " 'kilowatt',\n",
              " 'kilowatt hour',\n",
              " 'gigabyte',\n",
              " 'millimetre',\n",
              " 'pint',\n",
              " 'centilitre',\n",
              " 'candela',\n",
              " 'inch',\n",
              " 'person',\n",
              " 'metre',\n",
              " 'foot']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "VkAU-eq_Grr9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = OutputTokenizer(unit_list)"
      ],
      "metadata": {
        "id": "BBSMla-bHAgO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ProductImageDataset(train_df, tokenizer, max_length=50)\n",
        "val_dataset = ProductImageDataset(val_df, tokenizer, max_length=50)"
      ],
      "metadata": {
        "id": "y-13Cf-BHHoh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[3][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCyySsr-R7Fa",
        "outputId": "8cdca8f3-f0c1-404e-9cde-7e2064752335"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate(batch):\n",
        "    # Separate the batch into individual components\n",
        "    images, entity_names, group_ids, targets = zip(*batch)\n",
        "\n",
        "    # Stack images, entity_names, and group_ids (assuming they're already tensors of uniform size)\n",
        "    images = torch.stack(images, 0)\n",
        "    entity_names = torch.stack(entity_names, 0)\n",
        "    group_ids = torch.stack(group_ids, 0)\n",
        "\n",
        "    # Pad the target sequences\n",
        "    targets = pad_sequence(targets, batch_first=True, padding_value=0)\n",
        "\n",
        "    return images, entity_names, group_ids, targets"
      ],
      "metadata": {
        "id": "LIPD1XCvJZVs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4,collate_fn=custom_collate)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4,collate_fn=custom_collate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPyMGq03HNMj",
        "outputId": "ce8a552e-3e1d-4d9e-90d5-c0ddecdf1233"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise the Model\n",
        "model = EntityPredictorVisionBasedModel(\n",
        "    num_entity_names=len(entity_to_index),\n",
        "    num_group_ids=len(group_to_index),\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_length=50\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "Lyj5YpNsHXcS",
        "outputId": "d63a5e7e-55ad-4f27-8908-b3bcbd4fcf7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (img, entity_name, group_id, target) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
        "            img = img.to(device)\n",
        "            entity_name = entity_name.to(device)\n",
        "            group_id = group_id.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output_probs = model(img, entity_name, group_id, target)\n",
        "\n",
        "            # Reshape output_probs and target for loss calculation\n",
        "            output_probs = output_probs.view(-1, output_probs.size(-1))\n",
        "            target = target[:, 1:].contiguous().view(-1)  # Shift target by 1 and flatten\n",
        "\n",
        "            # Create a mask to ignore padding in loss calculation\n",
        "            mask = (target != 0).float()\n",
        "\n",
        "            loss = criterion(output_probs, target)\n",
        "            loss = (loss * mask).sum() / mask.sum()  # Average loss over non-pad tokens\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
        "                print(f\"Output probs shape: {output_probs.shape}\")\n",
        "                print(f\"Target shape: {target.shape}\")\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for img, entity_name, group_id, target in tqdm(val_loader, desc=\"Validation\"):\n",
        "                img = img.to(device)\n",
        "                entity_name = entity_name.to(device)\n",
        "                group_id = group_id.to(device)\n",
        "                target = target.to(device)\n",
        "\n",
        "                print(f\"Validation batch shapes:\")\n",
        "                print(f\"img: {img.shape}\")\n",
        "                print(f\"entity_name: {entity_name.shape}\")\n",
        "                print(f\"group_id: {group_id.shape}\")\n",
        "                print(f\"target: {target.shape}\")\n",
        "\n",
        "                output_probs = model(img, entity_name, group_id)\n",
        "                print(f\"output_probs: {output_probs.shape}\")\n",
        "\n",
        "                # Reshape output_probs to match target size\n",
        "                output_probs = output_probs.squeeze(1)  # Remove the extra dimension\n",
        "                output_probs = output_probs.view(-1, output_probs.size(-1))\n",
        "                target = target.view(-1)\n",
        "\n",
        "                print(f\"Reshaped output_probs: {output_probs.shape}\")\n",
        "                print(f\"Reshaped target: {target.shape}\")\n",
        "\n",
        "                # Create a mask to ignore padding in loss calculation\n",
        "                mask = (target != 0).float()\n",
        "\n",
        "                loss = criterion(output_probs, target)\n",
        "                loss = (loss * mask).sum() / mask.sum()  # Average loss over non-pad tokens\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Calculate accuracy\n",
        "                predictions = output_probs.argmax(dim=-1)\n",
        "                correct_predictions += ((predictions == target) * mask).sum().item()\n",
        "                total_predictions += mask.sum().item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        accuracy = correct_predictions / total_predictions\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ayhKYX4DK_f_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.PAD_token)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "jqO4PlSXHhrq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 3"
      ],
      "metadata": {
        "id": "J85wWNQkIPU4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "Z-HY9n7IIdyp",
        "outputId": "6b20b6b9-2087-44e0-e479-7103584c650b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3:   0%|          | 1/625 [00:09<1:36:04,  9.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0, Loss: 4.0404\n",
            "Output probs shape: torch.Size([1568, 46])\n",
            "Target shape: torch.Size([1568])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3:   1%|          | 7/625 [00:17<25:14,  2.45s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-f25c1317b44e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-62216139a637>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0moutput_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# Reshape output_probs and target for loss calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-ab3264af79b3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, entity_names, group_ids, target_texts)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mattended_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1273\u001b[0m                 is_causal=is_causal)\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1276\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5418\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_separate_proj_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5419\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0min_proj_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_separate_proj_weight is False but in_proj_weight is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5420\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_in_projection_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5421\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5422\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_proj_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_separate_proj_weight is True but q_proj_weight is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4929\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4930\u001b[0m                 \u001b[0mb_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_kv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4931\u001b[0;31m             \u001b[0mq_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4932\u001b[0m             \u001b[0mkv_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_kv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_kv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4933\u001b[0m             \u001b[0;31m# reshape to 2, E and not E, 2 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model"
      ],
      "metadata": {
        "id": "CopyrxWygeKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"model_v1_bs25k_no_1.pth\""
      ],
      "metadata": {
        "id": "hGRWuzBjgtjQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model saved to {save_path}\")"
      ],
      "metadata": {
        "id": "AXsBJMqdggAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation and dumping the Model"
      ],
      "metadata": {
        "id": "aP0J22qCfQml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import json\n",
        "\n",
        "def save_model_and_calculate_metrics(model, test_loader, tokenizer, device, save_path='trained_model.pth'):\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Disable gradient calculations\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images, entity_names, group_ids, targets = batch\n",
        "            images = images.to(device)\n",
        "            entity_names = entity_names.to(device)\n",
        "            group_ids = group_ids.to(device)\n",
        "\n",
        "            # Generate predictions\n",
        "            outputs = model.generate(images, entity_names, group_ids)\n",
        "\n",
        "            # Convert predictions and targets to text\n",
        "            pred_texts = [tokenizer.decode(pred) for pred in outputs.cpu().numpy()]\n",
        "            target_texts = [tokenizer.decode(target) for target in targets.cpu().numpy()]\n",
        "\n",
        "            all_predictions.extend(pred_texts)\n",
        "            all_targets.extend(target_texts)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_targets, all_predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_predictions, average='weighted')\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Save metrics to a file\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1\n",
        "    }\n",
        "    with open('metrics.json', 'w') as f:\n",
        "        json.dump(metrics, f, indent=4)\n",
        "    print(\"Metrics saved to metrics.json\")"
      ],
      "metadata": {
        "id": "0T62URf_JyA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Load your test dataset\n",
        "test_df = pd.read_csv('https://raw.githubusercontent.com/Dhyanesh-Panchal/amazon_ml_2024_deepinsight/refs/heads/master/student_resource%203/dataset/test.csv')\n",
        "test_df = test_df.sample(frac=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "aVfwxvgab2Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "FekDAJnsgCAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_oDYbsZFgK4n"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}